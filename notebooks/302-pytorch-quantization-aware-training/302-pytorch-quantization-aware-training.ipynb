{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "git68adWeq4l"
   },
   "source": [
    "# Intro\n",
    "\n",
    "This notebook is based on 'ImageNet training in PyTorch' [example](https://github.com/pytorch/examples/blob/master/imagenet/main.py).\n",
    "\n",
    "The goal of this notebook to demonstrate how to use [NNCF](https://github.com/openvinotoolkit/nncf) 8-bit quantization to optimize the PyTorch model for inference with OpenVINO Toolkit. For more advanced usage refer to these [examples](https://github.com/openvinotoolkit/nncf/tree/develop/examples)\n",
    "\n",
    "To make downloading and training fast, we use resnet-18 model with tiny-imagenet-200 dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6M1xndNu-z_2"
   },
   "source": [
    "# Settings\n",
    "Set the name for the model, and the image width and height that will be used for the network. Also define paths where PyTorch, ONNX and OpenVINO IR models will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BtaM_i2mEB0z",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.85 ms (started: 2021-08-07 15:28:11 +03:00)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "MODEL_DIR = Path('model')\n",
    "OUTPUT_DIR = Path('output')\n",
    "BASE_MODEL_NAME = 'resnet18'\n",
    "image_size = 64\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Paths where PyTorch, ONNX and OpenVINO IR models will be stored\n",
    "fp32_pth_path = Path(MODEL_DIR / (BASE_MODEL_NAME + '_fp32')).with_suffix('.pth') \n",
    "fp32_onnx_path = Path(OUTPUT_DIR / (BASE_MODEL_NAME + '_fp32')).with_suffix(\".onnx\")\n",
    "fp32_ir_path = fp32_onnx_path.with_suffix(\".xml\")\n",
    "int8_onnx_path = Path(OUTPUT_DIR / (BASE_MODEL_NAME + '_int8')).with_suffix('.onnx') \n",
    "int8_ir_path = int8_onnx_path.with_suffix(\".xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EIo5S145S0Ug",
    "outputId": "9a2db892-eb38-4863-dfdb-560aa12c8232",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Download Tiny ImageNet dataset\n",
    "* 100k images of shape 3x64x64\n",
    "* 200 different classes: snakes, spaiders, cats, trucks, grasshopper, gull, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-HxsU71bEbLS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15.1 ms (started: 2021-08-07 15:28:11 +03:00)\n"
     ]
    }
   ],
   "source": [
    "def download_tiny_imagenet_200(output_dir,\n",
    "                        url='http://cs231n.stanford.edu/tiny-imagenet-200.zip',\n",
    "                        tarname='tiny-imagenet-200.zip'):\n",
    "    from urllib.request import urlretrieve\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    archive_path = output_dir / tarname\n",
    "    urlretrieve(url, archive_path)\n",
    "    import zipfile\n",
    "    zip_ref = zipfile.ZipFile(archive_path, 'r')\n",
    "    zip_ref.extractall(path=output_dir)\n",
    "    zip_ref.close()\n",
    "    print(f'Successfully downloaded and extracted dataset to: {output_dir}')\n",
    "\n",
    "DATASET_DIR = OUTPUT_DIR / 'tiny-imagenet-200'\n",
    "if not DATASET_DIR.exists():\n",
    "    download_tiny_imagenet_200(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ya9XgA87DPoM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Install pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fx4lCGIGNp28"
   },
   "source": [
    "Create a separate Python* virtual environment and install the following prerequisites into it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gubHWwZELIRn",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nncf[torch] in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (2.0.0)\n",
      "Requirement already satisfied: wheel>=0.36.1 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from nncf[torch]) (0.36.2)\n",
      "Requirement already satisfied: jstyleson>=0.0.2 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from nncf[torch]) (0.0.2)\n",
      "Requirement already satisfied: scikit-learn>=0.24.0 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from nncf[torch]) (0.24.2)\n",
      "Requirement already satisfied: jsonschema==3.2.0 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from nncf[torch]) (3.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from nncf[torch]) (1.5.4)\n",
      "Requirement already satisfied: networkx>=2.5 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from nncf[torch]) (2.6.2)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from nncf[torch]) (1.19.5)\n",
      "Requirement already satisfied: ninja>=1.10.0.post2 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from nncf[torch]) (1.10.2)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from nncf[torch]) (1.1.5)\n",
      "Requirement already satisfied: addict>=2.4.0 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from nncf[torch]) (2.4.0)\n",
      "Requirement already satisfied: matplotlib>=3.3.4 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from nncf[torch]) (3.3.4)\n",
      "Requirement already satisfied: texttable>=1.6.3 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from nncf[torch]) (1.6.4)\n",
      "Requirement already satisfied: natsort>=7.1.0 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from nncf[torch]) (7.1.1)\n",
      "Requirement already satisfied: pydot>=1.4.1 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from nncf[torch]) (1.4.2)\n",
      "Requirement already satisfied: tqdm>=4.54.1 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from nncf[torch]) (4.62.0)\n",
      "Requirement already satisfied: torch!=1.8.0,<=1.8.1,>=1.5.0 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from nncf[torch]) (1.8.1)\n",
      "Requirement already satisfied: setuptools in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from jsonschema==3.2.0->nncf[torch]) (57.4.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from jsonschema==3.2.0->nncf[torch]) (1.15.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from jsonschema==3.2.0->nncf[torch]) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from jsonschema==3.2.0->nncf[torch]) (21.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from matplotlib>=3.3.4->nncf[torch]) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from matplotlib>=3.3.4->nncf[torch]) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from matplotlib>=3.3.4->nncf[torch]) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from matplotlib>=3.3.4->nncf[torch]) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from matplotlib>=3.3.4->nncf[torch]) (8.2.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from pandas>=1.1.5->nncf[torch]) (2021.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from scikit-learn>=0.24.0->nncf[torch]) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from scikit-learn>=0.24.0->nncf[torch]) (1.0.1)\n",
      "Requirement already satisfied: typing-extensions in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from torch!=1.8.0,<=1.8.1,>=1.5.0->nncf[torch]) (3.7.4.3)\n",
      "Requirement already satisfied: openvino in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (2021.4.0)\n",
      "Requirement already satisfied: numpy<1.20,>=1.16.6 in /home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages (from openvino) (1.19.5)\n",
      "time: 2.11 s (started: 2021-08-07 15:28:11 +03:00)\n"
     ]
    }
   ],
   "source": [
    "!pip install nncf[torch]\n",
    "!pip install openvino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4H9sI_uJOrT"
   },
   "source": [
    "# Import NNCF from your Python* code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q58vQSLWIwBp",
    "outputId": "ce194ed6-af92-48a4-90fe-18bd070b33ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "time: 33.3 s (started: 2021-08-07 15:28:13 +03:00)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import nncf  # Important - should be imported directly after torch\n",
    "from nncf import NNCFConfig\n",
    "from nncf.torch import create_compressed_model\n",
    "from nncf.torch import register_default_init_args\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZX2GAh3W7ZT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pre-train floating-point model\n",
    "Using NNCF for model compressed implies that the user has a pre-trained model and a training pipeline to get it.\n",
    "\n",
    "Here we demonstrate one of the possible training pipeline: the ResNet18 model pre-trained on 1000 classes of ImageNet is tuned on 200 classes of Tiny-Imagenet. \n",
    "\n",
    "Subsequently, the training and validation functions will be reused as is for quantization-aware training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E01dMaR2_AFL"
   },
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "940rcAIyiXml"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.39 ms (started: 2021-08-07 15:28:46 +03:00)\n"
     ]
    }
   ],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter('Time', ':3.3f')\n",
    "    losses = AverageMeter('Loss', ':2.3f')\n",
    "    top1 = AverageMeter('Acc@1', ':2.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':2.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, losses, top1, top5],\n",
    "        prefix=\"Epoch:[{}]\".format(epoch))\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # compute output\n",
    "        output = model(images)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        top1.update(acc1[0], images.size(0))\n",
    "        top5.update(acc5[0], images.size(0))\n",
    "\n",
    "        # compute gradient and do opt step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        print_frequency = 50\n",
    "        if i % print_frequency == 0:\n",
    "            progress.display(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CoNr8qwm_El2"
   },
   "source": [
    "## Validate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KgnugrWgicWC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 13.9 ms (started: 2021-08-07 15:28:46 +03:00)\n"
     ]
    }
   ],
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter('Time', ':3.3f')\n",
    "    losses = AverageMeter('Loss', ':2.3f')\n",
    "    top1 = AverageMeter('Acc@1', ':2.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':2.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses, top1, top5],\n",
    "        prefix='Test: ')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            top5.update(acc5[0], images.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            print_frequency = 10\n",
    "            if i % print_frequency == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "              .format(top1=top1, top5=top5))\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMnYsGo9_MA8"
   },
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "R724tbxcidQE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.05 ms (started: 2021-08-07 15:28:46 +03:00)\n"
     ]
    }
   ],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcSjyLBwiqBx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run training pipeline\n",
    "Tune floating-point ResNet-18 on Tiny-ImageNet-200 to obtain a pre-trained model for quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "118rlV22_PB5"
   },
   "source": [
    "> **NOTE** The model is not tuned till the final accuracy. For the sake of simplicity of the tutorial, we tune for 4 epoch only and take the last model state for quantization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "avCsioUYIaL7",
    "outputId": "183bdbb6-4016-463c-8d76-636a6b3a9778"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.57 s (started: 2021-08-07 15:28:46 +03:00)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 200  # 200 is for tiny-imagenet, default is 1000 for imagenet\n",
    "init_lr = 1e-4\n",
    "batch_size = 256\n",
    "epochs = 4\n",
    "\n",
    "# create model\n",
    "model = models.resnet18(pretrained=True)\n",
    "# update the last FC layer for tiny-imagenet number of classes\n",
    "model.fc = nn.Linear(in_features=512, out_features=num_classes, bias=True)\n",
    "model.to(device)\n",
    "\n",
    "# Data loading code\n",
    "train_dir = DATASET_DIR / 'train'\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225])\n",
    "\n",
    "dataset = datasets.ImageFolder(\n",
    "    train_dir,\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [80000, 20000], \n",
    "                                                           generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True,\n",
    "    num_workers=4, pin_memory=True, sampler=None)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False,\n",
    "    num_workers=4, pin_memory=True)\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "L0tH9KdwtHhV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of FP32 model: 53.235\n",
      "time: 29.3 ms (started: 2021-08-07 15:28:49 +03:00)\n"
     ]
    }
   ],
   "source": [
    "acc1 = 0\n",
    "best_acc1 = 0\n",
    "if fp32_pth_path.exists():\n",
    "    #\n",
    "    # ** WARNING: torch.load functionality uses Python's pickling facilities that\n",
    "    # may be used to perform arbitrary code execution during unpickling. Only load the data you\n",
    "    # trust.\n",
    "    #\n",
    "    checkpoint = torch.load(str(fp32_pth_path), map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['state_dict'], strict=True)\n",
    "    acc1 = checkpoint['acc1']\n",
    "else:\n",
    "    # Training loop\n",
    "    for epoch in range(0, epochs):\n",
    "        # run a single training epoch\n",
    "        train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        acc1 = validate(val_loader, model, criterion)\n",
    "\n",
    "        is_best = acc1 > best_acc1\n",
    "        best_acc1 = max(acc1, best_acc1)\n",
    "\n",
    "        if is_best:\n",
    "            checkpoint = {'state_dict': model.state_dict(), 'acc1': acc1}\n",
    "            torch.save(checkpoint, fp32_pth_path)\n",
    "                \n",
    "print(f'Accuracy of FP32 model: {acc1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pt_xNDDrJKsy",
    "outputId": "0925c801-0585-4431-98c9-de0decc4ad27",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Export FP32 model to ONNX that is supported by the OpenVINO™ toolkit to benchmark it in comparison with INT8 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9d8LOmKut36x",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 ONNX model was exported to output/resnet18_fp32.onnx.\n",
      "time: 505 ms (started: 2021-08-07 15:28:49 +03:00)\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 3, image_size, image_size).to(device)\n",
    "    \n",
    "torch.onnx.export(model, dummy_input, fp32_onnx_path)\n",
    "print(f\"FP32 ONNX model was exported to {fp32_onnx_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qk0Nub3KiqZK",
    "outputId": "a438341a-8211-4750-be34-440ae483a511",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Create and initialize quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pobVoHEoKcYp"
   },
   "source": [
    "NNCF enables compression-aware training by being integrated into the regular training pipelines. The framework is designed so that the modifications to your original training code are minor. \n",
    "Quantization is the simplest scenario and requires only 3 modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ENAbqFpdWSlE",
    "outputId": "cd2701e3-e4a2-4a19-86cd-ae37f45cd64a",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Configure NNCF parameters to specify compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_I_G-g9TPWkl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11.9 ms (started: 2021-08-07 15:28:49 +03:00)\n"
     ]
    }
   ],
   "source": [
    "nncf_config_dict = {\n",
    "    \"input_info\": {\n",
    "        \"sample_size\": [1, 3, image_size, image_size]\n",
    "    },\n",
    "    \"log_dir\": str(OUTPUT_DIR), # log directory for NNCF-specific logging outputs\n",
    "    \"compression\": {\n",
    "        \"algorithm\": \"quantization\",  # specify the algorithm here\n",
    "    }\n",
    "}\n",
    "nncf_config = NNCFConfig.from_dict(nncf_config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Provide data loader to initialize the values of quantization ranges  and determine which activation should be signed or unsigned from the collected statistics using given number of samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Please, provide execution parameters for optimal model initialization\n",
      "time: 879 ms (started: 2021-08-07 15:28:49 +03:00)\n"
     ]
    }
   ],
   "source": [
    "nncf_config = register_default_init_args(nncf_config, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a wrapped model ready for compression fine-tuning wrom a pre-trained FP32 model and a configuration object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:nncf:Graphviz is not installed - only the .dot model visualization format will be used. Install pygraphviz into your Python environment and graphviz system-wide to enable PNG rendering.\n",
      "INFO:nncf:Wrapping module ResNet/Conv2d[conv1] by ResNet/NNCFConv2d[conv1]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer1]/BasicBlock[0]/Conv2d[conv1] by ResNet/Sequential[layer1]/BasicBlock[0]/NNCFConv2d[conv1]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer1]/BasicBlock[0]/Conv2d[conv2] by ResNet/Sequential[layer1]/BasicBlock[0]/NNCFConv2d[conv2]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer1]/BasicBlock[1]/Conv2d[conv1] by ResNet/Sequential[layer1]/BasicBlock[1]/NNCFConv2d[conv1]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer1]/BasicBlock[1]/Conv2d[conv2] by ResNet/Sequential[layer1]/BasicBlock[1]/NNCFConv2d[conv2]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer2]/BasicBlock[0]/Conv2d[conv1] by ResNet/Sequential[layer2]/BasicBlock[0]/NNCFConv2d[conv1]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer2]/BasicBlock[0]/Conv2d[conv2] by ResNet/Sequential[layer2]/BasicBlock[0]/NNCFConv2d[conv2]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/Conv2d[0] by ResNet/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/NNCFConv2d[0]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer2]/BasicBlock[1]/Conv2d[conv1] by ResNet/Sequential[layer2]/BasicBlock[1]/NNCFConv2d[conv1]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer2]/BasicBlock[1]/Conv2d[conv2] by ResNet/Sequential[layer2]/BasicBlock[1]/NNCFConv2d[conv2]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer3]/BasicBlock[0]/Conv2d[conv1] by ResNet/Sequential[layer3]/BasicBlock[0]/NNCFConv2d[conv1]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer3]/BasicBlock[0]/Conv2d[conv2] by ResNet/Sequential[layer3]/BasicBlock[0]/NNCFConv2d[conv2]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/Conv2d[0] by ResNet/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/NNCFConv2d[0]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer3]/BasicBlock[1]/Conv2d[conv1] by ResNet/Sequential[layer3]/BasicBlock[1]/NNCFConv2d[conv1]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer3]/BasicBlock[1]/Conv2d[conv2] by ResNet/Sequential[layer3]/BasicBlock[1]/NNCFConv2d[conv2]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer4]/BasicBlock[0]/Conv2d[conv1] by ResNet/Sequential[layer4]/BasicBlock[0]/NNCFConv2d[conv1]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer4]/BasicBlock[0]/Conv2d[conv2] by ResNet/Sequential[layer4]/BasicBlock[0]/NNCFConv2d[conv2]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer4]/BasicBlock[0]/Sequential[downsample]/Conv2d[0] by ResNet/Sequential[layer4]/BasicBlock[0]/Sequential[downsample]/NNCFConv2d[0]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer4]/BasicBlock[1]/Conv2d[conv1] by ResNet/Sequential[layer4]/BasicBlock[1]/NNCFConv2d[conv1]\n",
      "INFO:nncf:Wrapping module ResNet/Sequential[layer4]/BasicBlock[1]/Conv2d[conv2] by ResNet/Sequential[layer4]/BasicBlock[1]/NNCFConv2d[conv2]\n",
      "INFO:nncf:Wrapping module ResNet/Linear[fc] by ResNet/NNCFLinear[fc]\n",
      "WARNING:nncf:Enabling quantization range initialization with default parameters.\n",
      "WARNING:nncf:NNCFNetwork(\n",
      "  (nncf_module): ResNet(\n",
      "    (conv1): NNCFConv2d(\n",
      "      3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "      (pre_ops): ModuleDict()\n",
      "      (post_ops): ModuleDict()\n",
      "    )\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): NNCFConv2d(\n",
      "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): NNCFConv2d(\n",
      "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): NNCFConv2d(\n",
      "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): NNCFConv2d(\n",
      "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): NNCFConv2d(\n",
      "          64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): NNCFConv2d(\n",
      "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): NNCFConv2d(\n",
      "            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (pre_ops): ModuleDict()\n",
      "            (post_ops): ModuleDict()\n",
      "          )\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): NNCFConv2d(\n",
      "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): NNCFConv2d(\n",
      "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): NNCFConv2d(\n",
      "          128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): NNCFConv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): NNCFConv2d(\n",
      "            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (pre_ops): ModuleDict()\n",
      "            (post_ops): ModuleDict()\n",
      "          )\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): NNCFConv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): NNCFConv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): NNCFConv2d(\n",
      "          256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): NNCFConv2d(\n",
      "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): NNCFConv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (pre_ops): ModuleDict()\n",
      "            (post_ops): ModuleDict()\n",
      "          )\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): NNCFConv2d(\n",
      "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): NNCFConv2d(\n",
      "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (pre_ops): ModuleDict()\n",
      "          (post_ops): ModuleDict()\n",
      "        )\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): NNCFLinear(\n",
      "      in_features=512, out_features=200, bias=True\n",
      "      (pre_ops): ModuleDict()\n",
      "      (post_ops): ModuleDict()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Collecting tensor statistics ████████████████  | 1 / 1\n",
      "INFO:nncf:Set sign: True and scale: [2.6400, ] for TargetType.OPERATOR_POST_HOOK /nncf_model_input_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK /nncf_model_input_0\n",
      "INFO:nncf:Set sign: False and scale: [3.5188, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer1]/BasicBlock[0]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer1]/BasicBlock[0]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Set sign: True and scale: [5.4515, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer1]/BasicBlock[0]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer1]/BasicBlock[0]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Set sign: False and scale: [2.5448, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer1]/BasicBlock[1]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer1]/BasicBlock[1]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Set sign: True and scale: [6.0996, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer1]/BasicBlock[1]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer1]/BasicBlock[1]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Set sign: False and scale: [3.1008, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[0]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[0]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Set sign: True and scale: [3.6365, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[0]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[0]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Set sign: True and scale: [3.3569, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1]/batch_norm_0\n",
      "INFO:nncf:Set sign: False and scale: [2.3807, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[1]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[1]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Set sign: True and scale: [3.5106, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[1]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[1]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Set sign: False and scale: [2.4122, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[0]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[0]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Set sign: True and scale: [3.7630, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[0]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[0]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Set sign: True and scale: [1.7134, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1]/batch_norm_0\n",
      "INFO:nncf:Set sign: False and scale: [2.1819, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[1]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[1]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Set sign: True and scale: [3.8630, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[1]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[1]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Set sign: False and scale: [1.8259, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[0]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[0]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Set sign: True and scale: [3.4402, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[0]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[0]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Set sign: True and scale: [2.0802, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1]/batch_norm_0\n",
      "INFO:nncf:Set sign: False and scale: [1.5971, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[1]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[1]/ReLU[relu]/relu__0\n",
      "INFO:nncf:Set sign: True and scale: [13.3573, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[1]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Performing signed activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[1]/BatchNorm2d[bn2]/batch_norm_0\n",
      "INFO:nncf:Set sign: False and scale: [13.9466, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[1]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[1]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Set sign: False and scale: [13.3258, ] for TargetType.OPERATOR_POST_HOOK ResNet/AdaptiveAvgPool2d[avgpool]/adaptive_avg_pool2d_0\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/AdaptiveAvgPool2d[avgpool]/adaptive_avg_pool2d_0\n",
      "INFO:nncf:Set sign: False and scale: [4.2417, ] for TargetType.OPERATOR_POST_HOOK ResNet/ReLU[relu]/relu__0\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/ReLU[relu]/relu__0\n",
      "INFO:nncf:Set sign: False and scale: [4.9304, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer1]/BasicBlock[0]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer1]/BasicBlock[0]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Set sign: False and scale: [5.7552, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer1]/BasicBlock[1]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer1]/BasicBlock[1]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Set sign: False and scale: [4.2934, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[0]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[0]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Set sign: False and scale: [4.7645, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[1]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer2]/BasicBlock[1]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Set sign: False and scale: [3.9678, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[0]/ReLU[relu]/relu__1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[0]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Set sign: False and scale: [5.5993, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[1]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer3]/BasicBlock[1]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Set sign: False and scale: [3.9081, ] for TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[0]/ReLU[relu]/relu__1\n",
      "INFO:nncf:Performing unsigned activation quantization for: TargetType.OPERATOR_POST_HOOK ResNet/Sequential[layer4]/BasicBlock[0]/ReLU[relu]/relu__1\n",
      "WARNING:nncf:The saturation issue fix will be applied. Now all weight quantizers will effectively use only 7 bits out of 8 bits. This resolves the saturation issue problem on AVX2 and AVX-512 machines. Please take a look at the documentation for a detailed information.\n",
      "INFO:nncf:Set sign: True and scale: [0.7697, 0.3192, 0.1000, 0.1759, 0.1000, 0.4172, 0.5834, 0.1000, 0.7551, 0.1000, ... (first 10/64 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.8057, 0.2424, 0.5567, 0.3693, 0.3543, 0.3245, 0.3739, 0.1844, 0.4633, 0.4254, ... (first 10/64 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer1]/BasicBlock[0]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer1]/BasicBlock[0]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.3296, 0.1498, 0.1000, 0.2334, 0.1321, 0.2323, 0.2458, 0.1831, 0.2766, 0.2608, ... (first 10/64 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer1]/BasicBlock[0]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer1]/BasicBlock[0]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.2264, 0.4283, 0.2280, 0.1708, 0.3042, 0.3131, 0.1995, 0.6516, 0.3480, 0.3617, ... (first 10/64 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer1]/BasicBlock[1]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer1]/BasicBlock[1]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1974, 0.3122, 0.1805, 0.1872, 0.1684, 0.3455, 0.2941, 0.2890, 0.2790, 0.2182, ... (first 10/64 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer1]/BasicBlock[1]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer1]/BasicBlock[1]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1751, 0.1536, 0.2079, 0.2128, 0.1923, 0.2390, 0.1268, 0.2419, 0.1381, 0.1761, ... (first 10/128 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer2]/BasicBlock[0]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer2]/BasicBlock[0]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.2277, 0.1510, 0.2501, 0.1147, 0.2240, 0.1992, 0.1876, 0.1672, 0.1803, 0.1529, ... (first 10/128 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer2]/BasicBlock[0]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer2]/BasicBlock[0]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.4994, 0.1158, 0.1705, 0.3253, 0.1735, 0.1507, 0.1711, 0.3948, 0.1650, 0.3842, ... (first 10/128 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/NNCFConv2d[0]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/NNCFConv2d[0]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.2603, 0.1477, 0.1656, 0.1719, 0.2057, 0.2082, 0.1157, 0.2736, 0.2245, 0.2921, ... (first 10/128 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer2]/BasicBlock[1]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer2]/BasicBlock[1]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1350, 0.1000, 0.1554, 0.1801, 0.1467, 0.1267, 0.1297, 0.1296, 0.1075, 0.1730, ... (first 10/128 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer2]/BasicBlock[1]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer2]/BasicBlock[1]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1748, 0.2837, 0.1333, 0.1774, 0.1172, 0.1427, 0.1237, 0.1293, 0.1278, 0.1099, ... (first 10/256 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer3]/BasicBlock[0]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer3]/BasicBlock[0]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1263, 0.1105, 0.1000, 0.1835, 0.1286, 0.1917, 0.1617, 0.1074, 0.1594, 0.1212, ... (first 10/256 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer3]/BasicBlock[0]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer3]/BasicBlock[0]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1231, 0.1069, 0.1000, 0.1602, 0.1055, 0.1290, 0.1011, 0.1006, 0.1000, 0.1000, ... (first 10/256 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/NNCFConv2d[0]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/NNCFConv2d[0]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1070, 0.2032, 0.1125, 0.2060, 0.1021, 0.1053, 0.2286, 0.1581, 0.1000, 0.1262, ... (first 10/256 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer3]/BasicBlock[1]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer3]/BasicBlock[1]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1000, 0.1317, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, ... (first 10/256 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer3]/BasicBlock[1]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer3]/BasicBlock[1]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1046, 0.1000, 0.1166, 0.1000, 0.1000, 0.1000, 0.1000, 0.1315, 0.1000, 0.1356, ... (first 10/512 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer4]/BasicBlock[0]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer4]/BasicBlock[0]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1130, 0.1029, 0.1000, 0.1000, 0.1035, 0.1000, 0.1252, 0.1065, 0.1000, 0.1227, ... (first 10/512 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer4]/BasicBlock[0]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer4]/BasicBlock[0]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1000, 0.1292, 0.1000, 0.2748, 0.1000, 0.1032, 0.1426, 0.2061, 0.1000, 0.1048, ... (first 10/512 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer4]/BasicBlock[0]/Sequential[downsample]/NNCFConv2d[0]/conv2d_0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer4]/BasicBlock[0]/Sequential[downsample]/NNCFConv2d[0]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1019, 0.1000, 0.1000, ... (first 10/512 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer4]/BasicBlock[1]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer4]/BasicBlock[1]/NNCFConv2d[conv1]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1317, 0.1000, 0.1000, ... (first 10/512 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer4]/BasicBlock[1]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/Sequential[layer4]/BasicBlock[1]/NNCFConv2d[conv2]/conv2d_0\n",
      "INFO:nncf:Set sign: True and scale: [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, ... (first 10/200 elements shown only) ] for TargetType.OPERATION_WITH_WEIGHTS ResNet/NNCFLinear[fc]/linear_0\n",
      "INFO:nncf:Performing signed weight quantization for: TargetType.OPERATION_WITH_WEIGHTS ResNet/NNCFLinear[fc]/linear_0\n",
      "INFO:nncf:BatchNorm statistics adaptation ██                | 1 / 8\n",
      "INFO:nncf:BatchNorm statistics adaptation ████              | 2 / 8\n",
      "INFO:nncf:BatchNorm statistics adaptation ██████            | 3 / 8\n",
      "INFO:nncf:BatchNorm statistics adaptation ████████          | 4 / 8\n",
      "INFO:nncf:BatchNorm statistics adaptation ██████████        | 5 / 8\n",
      "INFO:nncf:BatchNorm statistics adaptation ████████████      | 6 / 8\n",
      "INFO:nncf:BatchNorm statistics adaptation ██████████████    | 7 / 8\n",
      "INFO:nncf:BatchNorm statistics adaptation ████████████████  | 8 / 8\n",
      "WARNING:nncf:Graphviz is not installed - only the .dot model visualization format will be used. Install pygraphviz into your Python environment and graphviz system-wide to enable PNG rendering.\n",
      "time: 2.36 s (started: 2021-08-07 15:28:50 +03:00)\n"
     ]
    }
   ],
   "source": [
    "compression_ctrl, model = create_compressed_model(model, nncf_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the obtained model on the validation set after initialization of quantization. The accuracy should be close to the accuracy of the floating-point model in a simple case like we are demonstrating now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [ 0/79]\tTime 0.325 (0.325)\tLoss 2.006 (2.006)\tAcc@1 55.47 (55.47)\tAcc@5 80.47 (80.47)\n",
      "Test: [10/79]\tTime 0.046 (0.075)\tLoss 1.634 (1.903)\tAcc@1 59.77 (53.69)\tAcc@5 82.42 (79.30)\n",
      "Test: [20/79]\tTime 0.041 (0.061)\tLoss 1.832 (1.882)\tAcc@1 55.08 (54.30)\tAcc@5 80.47 (79.78)\n",
      "Test: [30/79]\tTime 0.037 (0.056)\tLoss 1.868 (1.879)\tAcc@1 55.86 (54.36)\tAcc@5 76.56 (79.60)\n",
      "Test: [40/79]\tTime 0.052 (0.054)\tLoss 1.828 (1.869)\tAcc@1 53.52 (54.31)\tAcc@5 80.86 (79.84)\n",
      "Test: [50/79]\tTime 0.066 (0.053)\tLoss 1.961 (1.873)\tAcc@1 50.39 (54.22)\tAcc@5 82.03 (79.91)\n",
      "Test: [60/79]\tTime 0.037 (0.052)\tLoss 1.746 (1.882)\tAcc@1 55.86 (54.01)\tAcc@5 80.86 (79.64)\n",
      "Test: [70/79]\tTime 0.055 (0.051)\tLoss 1.907 (1.884)\tAcc@1 55.86 (53.93)\tAcc@5 78.52 (79.52)\n",
      " * Acc@1 53.800 Acc@5 79.565\n",
      "Accuracy of initialized INT8 model: 53.800\n",
      "time: 3.98 s (started: 2021-08-07 15:28:52 +03:00)\n"
     ]
    }
   ],
   "source": [
    "acc1 = validate(val_loader, model, criterion)\n",
    "print(f'Accuracy of initialized INT8 model: {acc1:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune the compressed model\n",
    "\n",
    "At this step, a regular fine-tuning process is applied to restore accuracy drop. Normally, it is required several epochs of tuning with a small learning rate, the same that is usually used at the end of the training of the original model. No other changes in the training pipeline are required. Here is the simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[4][  0/313]\tTime 0.360 (0.360)\tLoss 0.973 (0.973)\tAcc@1 78.91 (78.91)\tAcc@5 90.23 (90.23)\n",
      "Epoch:[4][ 50/313]\tTime 0.083 (0.093)\tLoss 0.972 (0.938)\tAcc@1 78.91 (77.26)\tAcc@5 90.62 (92.92)\n",
      "Epoch:[4][100/313]\tTime 0.083 (0.090)\tLoss 1.131 (0.951)\tAcc@1 69.53 (76.93)\tAcc@5 90.62 (92.74)\n",
      "Epoch:[4][150/313]\tTime 0.089 (0.089)\tLoss 0.940 (0.953)\tAcc@1 75.78 (76.74)\tAcc@5 92.97 (92.69)\n",
      "Epoch:[4][200/313]\tTime 0.081 (0.089)\tLoss 0.911 (0.958)\tAcc@1 77.34 (76.63)\tAcc@5 91.80 (92.55)\n",
      "Epoch:[4][250/313]\tTime 0.087 (0.089)\tLoss 0.994 (0.959)\tAcc@1 76.56 (76.47)\tAcc@5 91.41 (92.47)\n",
      "Epoch:[4][300/313]\tTime 0.085 (0.089)\tLoss 0.909 (0.963)\tAcc@1 77.34 (76.27)\tAcc@5 92.58 (92.42)\n",
      "Test: [ 0/79]\tTime 0.359 (0.359)\tLoss 2.133 (2.133)\tAcc@1 50.78 (50.78)\tAcc@5 76.56 (76.56)\n",
      "Test: [10/79]\tTime 0.040 (0.074)\tLoss 1.627 (1.924)\tAcc@1 61.72 (53.27)\tAcc@5 82.42 (78.98)\n",
      "Test: [20/79]\tTime 0.051 (0.060)\tLoss 1.817 (1.895)\tAcc@1 53.91 (54.15)\tAcc@5 84.38 (79.65)\n",
      "Test: [30/79]\tTime 0.038 (0.055)\tLoss 1.986 (1.908)\tAcc@1 55.08 (54.23)\tAcc@5 76.95 (79.30)\n",
      "Test: [40/79]\tTime 0.040 (0.054)\tLoss 1.873 (1.904)\tAcc@1 53.91 (54.38)\tAcc@5 79.69 (79.33)\n",
      "Test: [50/79]\tTime 0.049 (0.053)\tLoss 1.951 (1.907)\tAcc@1 51.56 (54.34)\tAcc@5 78.12 (79.32)\n",
      "Test: [60/79]\tTime 0.041 (0.052)\tLoss 1.830 (1.917)\tAcc@1 56.64 (53.96)\tAcc@5 80.47 (79.23)\n",
      "Test: [70/79]\tTime 0.037 (0.051)\tLoss 1.906 (1.917)\tAcc@1 55.08 (54.04)\tAcc@5 76.95 (79.16)\n",
      " * Acc@1 53.925 Acc@5 79.265\n",
      "Accuracy of tuned INT8 model: 53.925\n",
      "time: 31.7 s (started: 2021-08-07 15:28:56 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# train for one epoch with NNCF\n",
    "train(train_loader, model, criterion, optimizer, epoch=epochs)\n",
    "\n",
    "# evaluate on validation set after Quantization-Aware Training (QAT case)\n",
    "acc1 = validate(val_loader, model, criterion)\n",
    "\n",
    "print(f'Accuracy of tuned INT8 model: {acc1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export INT8 model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages/torch/onnx/utils.py:305: UserWarning: It is recommended that constant folding be turned off ('do_constant_folding=False') when exporting the model in training-amenable mode, i.e. with 'training=TrainingMode.TRAIN' or 'training=TrainingMode.PRESERVE' (when model is in training mode). Otherwise, some learnable model parameters may not translate correctly in the exported ONNX model because constant folding mutates model parameters. Please consider turning off constant folding or setting the training=TrainingMode.EVAL.\n",
      "  warnings.warn(\"It is recommended that constant folding be turned off ('do_constant_folding=False') \"\n",
      "/home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/trace_tensor.py:36: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  self.shape = tuple(int(dim) for dim in shape)  # Handle cases when shape is a tuple of Tensors\n",
      "/home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages/nncf/torch/quantization/layers.py:148: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not self.is_enabled_quantization():\n",
      "/home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages/nncf/torch/quantization/layers.py:204: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return self._num_bits.item()\n",
      "/home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages/nncf/torch/quantization/layers.py:414: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return self.signed_tensor.item() == 1\n",
      "/home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages/nncf/torch/layers.py:100: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  self.get_padding_value_ref().data.fill_(padding_value.item())\n",
      "/home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages/nncf/torch/layers.py:106: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not self.get_padding_value_ref():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INT8 ONNX model exported to output/resnet18_int8.onnx.\n",
      "time: 853 ms (started: 2021-08-07 15:29:28 +03:00)\n"
     ]
    }
   ],
   "source": [
    "if not int8_onnx_path.exists():\n",
    "    # Export INT8 model to ONNX that is supported by the OpenVINO™ toolkit\n",
    "    compression_ctrl.export_model(int8_onnx_path)\n",
    "    print(f\"INT8 ONNX model exported to {int8_onnx_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export ONNX models to OpenVINO™ Intermediate Representation (IR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the OpenVINO Model Optimizer tool to convert the ONNX model to OpenVINO IR, with FP16 precision. The models are saved to the current directory. We add the mean values to the model and scale the output with the standard deviation by --mean_values and --scale_values arguments. It is not necessary to normalize input data before propagating it through the network with these options.\n",
    "\n",
    "See the [Model Optimizer Developer Guide](https://docs.openvinotoolkit.org/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) for more information about Model Optimizer.\n",
    "\n",
    "Executing this command may take a while. There may be some errors or warnings in the output. Model Optimization succesfully export to IR if the last lines of the output include: `[ SUCCESS ] Generated IR version 10 model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/home/nlyalyus/Developer/projects/openvino_notebooks/notebooks/302-pytorch-quantization-aware-training/output/resnet18_fp32.onnx\n",
      "\t- Path for generated IR: \t/home/nlyalyus/Developer/projects/openvino_notebooks/notebooks/302-pytorch-quantization-aware-training/output\n",
      "\t- IR output name: \tresnet18_fp32\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \t[1,3, 64, 64]\n",
      "\t- Mean values: \t[123.675, 116.28 , 103.53]\n",
      "\t- Scale values: \t[58.395, 57.12 , 57.375]\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP16\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tNone\n",
      "\t- Reverse input channels: \tFalse\n",
      "ONNX specific parameters:\n",
      "\t- Inference Engine found in: \t/home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages/openvino\n",
      "Inference Engine version: \t2021.4.0-3839-cd81789d294-releases/2021/4\n",
      "Model Optimizer version: \t2021.4.0-3839-cd81789d294-releases/2021/4\n",
      "[ SUCCESS ] Generated IR version 10 model.\n",
      "[ SUCCESS ] XML file: /home/nlyalyus/Developer/projects/openvino_notebooks/notebooks/302-pytorch-quantization-aware-training/output/resnet18_fp32.xml\n",
      "[ SUCCESS ] BIN file: /home/nlyalyus/Developer/projects/openvino_notebooks/notebooks/302-pytorch-quantization-aware-training/output/resnet18_fp32.bin\n",
      "[ SUCCESS ] Total execution time: 2.64 seconds. \n",
      "[ SUCCESS ] Memory consumed: 213 MB. \n",
      "time: 3.24 s (started: 2021-08-07 15:29:29 +03:00)\n"
     ]
    }
   ],
   "source": [
    "if not fp32_ir_path.exists():\n",
    "    !mo --input_model $fp32_onnx_path --input_shape \"[1,3, $image_size, $image_size]\" --mean_values \"[123.675, 116.28 , 103.53]\" --scale_values \"[58.395, 57.12 , 57.375]\" --data_type FP16 --output_dir $OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/home/nlyalyus/Developer/projects/openvino_notebooks/notebooks/302-pytorch-quantization-aware-training/output/resnet18_int8.onnx\n",
      "\t- Path for generated IR: \t/home/nlyalyus/Developer/projects/openvino_notebooks/notebooks/302-pytorch-quantization-aware-training/output\n",
      "\t- IR output name: \tresnet18_int8\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \t[1,3, 64, 64]\n",
      "\t- Mean values: \t[123.675, 116.28 , 103.53]\n",
      "\t- Scale values: \t[58.395, 57.12 , 57.375]\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP16\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tNone\n",
      "\t- Reverse input channels: \tFalse\n",
      "ONNX specific parameters:\n",
      "\t- Inference Engine found in: \t/home/nlyalyus/Developer/env/openvino_env/lib/python3.8/site-packages/openvino\n",
      "Inference Engine version: \t2021.4.0-3839-cd81789d294-releases/2021/4\n",
      "Model Optimizer version: \t2021.4.0-3839-cd81789d294-releases/2021/4\n",
      "[ SUCCESS ] Generated IR version 10 model.\n",
      "[ SUCCESS ] XML file: /home/nlyalyus/Developer/projects/openvino_notebooks/notebooks/302-pytorch-quantization-aware-training/output/resnet18_int8.xml\n",
      "[ SUCCESS ] BIN file: /home/nlyalyus/Developer/projects/openvino_notebooks/notebooks/302-pytorch-quantization-aware-training/output/resnet18_int8.bin\n",
      "[ SUCCESS ] Total execution time: 8.15 seconds. \n",
      "[ SUCCESS ] Memory consumed: 324 MB. \n",
      "time: 8.78 s (started: 2021-08-07 15:29:32 +03:00)\n"
     ]
    }
   ],
   "source": [
    "if not int8_ir_path.exists():\n",
    "    !mo --input_model $int8_onnx_path --input_shape \"[1,3, $image_size, $image_size]\" --mean_values \"[123.675, 116.28 , 103.53]\" --scale_values \"[58.395, 57.12 , 57.375]\" --data_type FP16 --output_dir $OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark model performance by computing inference time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will measure the inference performance of the FP32 and INT8 models. To do this, we use [Benchmark Tool](https://docs.openvinotoolkit.org/latest/openvino_inference_engine_tools_benchmark_tool_README.html) - OpenVINO's inference performance measurement tool. By default, Benchmark Tool runs inference for 60 seconds in asynchronous mode on CPU. It returns inference speed as latency (milliseconds per image) and throughput (frames per second) values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: In this notebook we run benchmark_app for 15 seconds to give a quick indication of performance. For more accurate performance, we recommended running benchmark_app in a terminal/command prompt after closing other applications. Run benchmark_app -m model.xml -d CPU to benchmark async inference on CPU for one minute. Change CPU to GPU to benchmark on GPU. Run benchmark_app --help to see an overview of all command line options.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark FP32 model (IR)\n",
      "Count:      41544 iterations\n",
      "Duration:   15002.82 ms\n",
      "Latency:    2.06 ms\n",
      "Throughput: 2769.08 FPS\n",
      "Benchmark INT8 model (IR)\n"
     ]
    }
   ],
   "source": [
    "def parse_benchmark_output(benchmark_output):\n",
    "    parsed_output = [line for line in benchmark_output if not (line.startswith(r\"[\") or line.startswith(\"  \") or line==\"\")]\n",
    "    print(*parsed_output, sep='\\n')\n",
    "\n",
    "print(f'Benchmark FP32 model (IR)')\n",
    "benchmark_output = ! benchmark_app -m $fp32_ir_path -d CPU -api async -t 15\n",
    "parse_benchmark_output(benchmark_output)\n",
    "\n",
    "print(f'Benchmark INT8 model (IR)')\n",
    "benchmark_output = ! benchmark_app -m $int8_ir_path -d CPU -api async -t 15\n",
    "parse_benchmark_output(benchmark_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show CPU Information for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import cpuinfo\n",
    "    print(cpuinfo.get_cpu_info()[\"brand_raw\"])\n",
    "except Exception:\n",
    "    # OpenVINO installs cpuinfo, but if a different version is installed\n",
    "    # the command above may not work\n",
    "    import platform\n",
    "    print(platform.processor())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "K5HPrY_d-7cV",
    "E01dMaR2_AFL",
    "qMnYsGo9_MA8",
    "L0tH9KdwtHhV"
   ],
   "name": "NNCF Quantization PyTorch Demo (tiny-imagenet/resnet-18",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
